# Syllabus BreakDown

## UNIT 1: INTRODUCTION TO NEURAL NETWORKS

_**1.1 Biological Neuron Model**_

- Structure of biological neuron (dendrites, soma, axon, synapse)
- Signal transmission mechanism
- Comparison with artificial neuron
- Motivation for ANN

_**1.2 Artificial Neuron (McCullochâ€“Pitts Model)**_

- Mathematical model of artificial neuron
- Weighted sum and bias
- Threshold logic unit (TLU)
- Inputâ€“output relationship

_**1.3 Characteristics of Artificial Neural Networks**_

- Non-linearity
- Adaptive learning
- Generalization capability
- Fault tolerance
- Parallelism and distributed processing
- Comparison with conventional computing

_**1.4 Types of Activation Functions**_

- Binary step function
- Linear activation function
- Sigmoid (logistic)
- Tanh
- ReLU and variants
- Mathematical properties and graphs
- Role in learning and convergence

ðŸ“Œ _Exam Focus_: Definitions, diagrams, activation function graphs, small numericals.

---

## UNIT 2: PERCEPTRONS & ANN PARADIGMS

_**2.1 Perceptron Model**_

- Single neuron perceptron
- Mathematical representation
- Geometry of decision boundary

_**2.2 Linear Separability**_

- Concept of linearly separable data
- XOR problem
- Geometric interpretation
- Proof of limitation

_**2.3 Single-Layer Perceptron**_

- Architecture
- Capabilities and constraints
- Classification examples

_**2.4 Multi-Layer Perceptron (MLP)**_

- Architecture overview
- Role of hidden layers
- Why multilayer networks solve XOR
- Comparison with single-layer perceptron

_**2.5 Perceptron Learning Algorithm**_

- Learning rule
- Weight update equation
- Convergence theorem (statement)
- Step-by-step numerical example

_**2.6 ANN Paradigms (Associative & Competitive Networks)**_

- Overview of ANN paradigms
- Supervised vs unsupervised paradigms

_**2.7 Hopfield Network**_

- Architecture
- Energy function
- Stability concept
- Applications (associative memory)

_**2.8 Hamming Network**_

- Architecture
- Pattern classification principle

_**2.9 Grossberg / Carpenter Network (ART)**_

- Adaptive Resonance Theory (ART)
- Stabilityâ€“plasticity dilemma

_**2.10 Kohonen Self-Organizing Map (SOM)**_

- Competitive learning
- Neighborhood function
- Feature mapping

ðŸ“Œ _Exam Focus_: XOR proof, learning algorithm numericals, network diagrams.

---

## UNIT 3: LEARNING MECHANISMS IN ANNs

_**3.1 Learning in Neural Networks**_

- Definition of learning
- Error surface concept
- Cost / loss function

_**3.2 Supervised Learning**_

- Training data and target outputs
- Error correction learning
- Examples: Perceptron, MLP

_**3.3 Backpropagation Algorithm**_

- Forward pass and backward pass
- Gradient descent
- Chain rule derivation
- Weight update equations
- Numerical example (mandatory for exams)
- Vanishing gradient (intro level)

_**3.4 Unsupervised Learning**_

- Concept and motivation
- Clustering vs classification
- Hebbian learning rule

_**3.5 Self-Organization**_

- Competitive learning
- Feature extraction
- Kohonen SOM (connection with Unit 2)

_**3.6 Counter Propagation Network**_

- Architecture (Kohonen + Grossberg layers)
- Learning phases
- Applications

ðŸ“Œ _Exam Focus_: Backprop derivation, numericals, learning rule comparisons.

---

## UNIT 4: HOPFIELD NETWORKS & APPLICATIONS

_**4.1 Hopfield Network Configuration**_

- Fully connected recurrent network
- Symmetric weight matrix
- Binary vs bipolar representation

_**4.2 Energy Function in Hopfield Networks**_

- Lyapunov function
- Proof of convergence to stable state
- Mathematical interpretation

_**4.3 Learning in Hopfield Networks**_

- Hebbian learning
- Weight calculation
- Storage capacity

_**4.4 Hardware Implementation of ANN**_

- Analog vs digital implementation
- VLSI perspective (introductory)
- Neuromorphic concepts (basic)

_**4.5 Emerging Applications of ANNs**_

- Pattern recognition
- Image and speech processing
- Medical diagnosis
- Control systems
- Forecasting and optimization

ðŸ“Œ _Exam Focus_: Energy function proof, weight matrix numericals, applications.
