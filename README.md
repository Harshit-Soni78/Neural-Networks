# NEURAL NETWORKS

**Unit 1:** Introduction: Biological Neurons and Synapses, Characteristics of Artificial Neural Networks (ANN); Types of Activation Functions.

**Unit 2:** Perceptrons: Perceptrons representation, Concept of linear separability, Limitations of Perceptrons Single layer and multilayer Perceptrons, Perceptron learning algorithms, Introduction to ANN paradigms, Hopfield net, Hamming net, Grossberg / Carpenter net, Kohonen's net.

**Unit 3:** Basic concept of Learning in ANNs: Supervised learning, backpropagation, Unsupervised learning, self- organization, Counter propagation.

**Unit 4:** Hopfield Networks: Network configuration, Hardware Implementation; Learning, Engorging Applications of ANN's.

_**Reference Books**_

1. L.V. Fausett, Artificial Neural Networks, Pearson Education
2. J.M. Zurada, Introduction to Artificial Neural System, Jaico Publisher
3. P.D. Wasserman, Neural Computing Theory and Practice, Coriolis Group
4. Simon Haykin, Neural Networks and Learning Machines, PHI

## Material

**Project Link:** <https://chatgpt.com/g/g-p-6942d67f2efc81919f4d4abda87a25d5-pinkynn/project>

### UNIT 1: INTRODUCTION TO NEURAL NETWORKS

_**1.1 Biological Neuron Model**_

- Structure of biological neuron (dendrites, soma, axon, synapse)
- Signal transmission mechanism
- Comparison with artificial neuron
- Motivation for ANN

_**1.2 Artificial Neuron (McCullochâ€“Pitts Model)**_

- Mathematical model of artificial neuron
- Weighted sum and bias
- Threshold logic unit (TLU)
- Inputâ€“output relationship

_**1.3 Characteristics of Artificial Neural Networks**_

- Non-linearity
- Adaptive learning
- Generalization capability
- Fault tolerance
- Parallelism and distributed processing
- Comparison with conventional computing

_**1.4 Types of Activation Functions**_

- Binary step function
- Linear activation function
- Sigmoid (logistic)
- Tanh
- ReLU and variants
- Mathematical properties and graphs
- Role in learning and convergence

ðŸ“Œ _Exam Focus_: Definitions, diagrams, activation function graphs, small numericals.

_**Links**_

- **ChatGPT:** <https://chatgpt.com/share/6942ddcc-29c8-8012-ba96-cd990d984343>

---

### UNIT 2: PERCEPTRONS & ANN PARADIGMS

_**2.1 Perceptron Model**_

- Single neuron perceptron
- Mathematical representation
- Geometry of decision boundary

_**2.2 Linear Separability**_

- Concept of linearly separable data
- XOR problem
- Geometric interpretation
- Proof of limitation

_**2.3 Single-Layer Perceptron**_

- Architecture
- Capabilities and constraints
- Classification examples

_**2.4 Multi-Layer Perceptron (MLP)**_

- Architecture overview
- Role of hidden layers
- Why multilayer networks solve XOR
- Comparison with single-layer perceptron

_**2.5 Perceptron Learning Algorithm**_

- Learning rule
- Weight update equation
- Convergence theorem (statement)
- Step-by-step numerical example

_**2.6 ANN Paradigms (Associative & Competitive Networks)**_

- Overview of ANN paradigms
- Supervised vs unsupervised paradigms

_**2.7 Hopfield Network**_

- Architecture
- Energy function
- Stability concept
- Applications (associative memory)

_**2.8 Hamming Network**_

- Architecture
- Pattern classification principle

_**2.9 Grossberg / Carpenter Network (ART)**_

- Adaptive Resonance Theory (ART)
- Stabilityâ€“plasticity dilemma

_**2.10 Kohonen Self-Organizing Map (SOM)**_

- Competitive learning
- Neighborhood function
- Feature mapping

ðŸ“Œ _Exam Focus_: XOR proof, learning algorithm numericals, network diagrams.

_**Links**_

- **ChatGPT:** <https://chatgpt.com/share/6942faec-faec-8012-b126-6a9509e75c8d>

---

### UNIT 3: LEARNING MECHANISMS IN ANNs

_**3.1 Learning in Neural Networks**_

- Definition of learning
- Error surface concept
- Cost / loss function

_**3.2 Supervised Learning**_

- Training data and target outputs
- Error correction learning
- Examples: Perceptron, MLP

_**3.3 Backpropagation Algorithm**_

- Forward pass and backward pass
- Gradient descent
- Chain rule derivation
- Weight update equations
- Numerical example (mandatory for exams)
- Vanishing gradient (intro level)

_**3.4 Unsupervised Learning**_

- Concept and motivation
- Clustering vs classification
- Hebbian learning rule

_**3.5 Self-Organization**_

- Competitive learning
- Feature extraction
- Kohonen SOM (connection with Unit 2)

_**3.6 Counter Propagation Network**_

- Architecture (Kohonen + Grossberg layers)
- Learning phases
- Applications

ðŸ“Œ _Exam Focus_: Backprop derivation, numericals, learning rule comparisons.

_**Links**_

- **ChatGPT:** <https://chatgpt.com/share/6942fb08-3994-8012-99fb-21ef5625fcb3>

---
